\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Rookie facets}
\author{}
\date{January 2016}


\usepackage{flexisym}

\begin{document}

\maketitle

\section{Binning}

TODO: UPDATE

Traditional search engines help users find particular information. Exploratory search engines help users get an overall broad sense of data. This poses a design challenge. If Rookie's goal is to help reporters, researchers and readers better understand broad stories in current events, while still allowing users to drill down to particlar documents: how should Rookie present document lists?

organize results by relevance. Rookie organizes results temporally. \footnote{what is case for why?} This poses as design challenge. If a query query returns more results than a user can read quickly, how should rookie 

Scrolling?


\subsection{Index time}



\section{Rookie facet engine}

When a user enters a query, Q, into the Rookie UI, Rookie executes a traditional document search using an information retrieval system\footnote {the current implmentation uses Whoosh} to find a set of query-responsive documents, D. In some cases, Rookie may filter D into a subset of documents which fall within a specified timespan, T. Regardless, Rookie passes either the set of documents or the set of time-filtered documents to its facet engine.

\subsection{Index time}

The Rookie facet engine makes use of a binary matrix, M, constructed at index time. The rows of M represent each part of part-of-speech filtered ngram type \footnote{Earlier versions of Rookie selected people, organizations and ngrams to ensure facet diversity. But this proved unnecessary} that occurs at least 5 times across the entire corpus. The columns of M represent all documents in the corpus. A value of 1 at position $M_{i,j}$ indicates that the i\textsuperscript{th} ngram type occurs in the j\textsuperscript{th} document. (Similarly, a value of 0 indicates that the ngram does not occur in the document).

Rookie also creates a vector of ``inverse document frequencies'', $\vec{idf}$, for each ngram type at index time. Each component of the vector, $idf_{i}$, represents the inverse document frequency for a particular ngram, which is computed as follows:

\begin{equation}
idf_{i} = ln(N/df_i)
\end{equation}

\noindent where N is the number of documents in the corpus and $df_{i}$ is the number of documents in the corpus that contain the ngram type. For example, if there are 10 documents in the corpus and 2 contain the i\textsuperscript{th} ngram, the inverse document frequency for that ngram would be $ln(10/2)$.

\subsection{Query time}

At query time, Rookie creates a submatrix from M, picking those columns which respresent the documents in D. It then sums the rows of the submatrix to generate a summary vector of ``term frequencies'', $\vec{tf}$. The i\textsuperscript{th} component of this vector, $tf_{i}$ represents how many times a given ngram type occurs within the documents in D. 

Element-wise multiplication of $\vec{tf}$ and $\vec{idf}$ produces a vector of tfidf scores, $\vec{S}$  for each ngram type.

\begin{equation}
\vec{S} = \vec{tf} \odot \vec{idf}
\end{equation}

Rookie selects highest-scoring ngrams from $\vec{S}$ until it reaches a fixed number of facets \footnote{TODO: character budget}, passed as a parameter in the call to the facet algorithm. However, because top-scoring facets often use slightly different string tokens to refer to the same person, organization or concept (shown in table \ref{t1:one}) \footnote{TODO: Don't understand why exactly this is happening. POS tagging errors? Linguistic variation in language in articles? Refer to different things?}
Rookie uses heuristics to clean up facets for presentation during its greedy search.

Table \ref{t1:one} shows the 15 highest ranking facets for Q=``Mitch Landrieu'', with and without heuristic cleanup. The top three raw facets are all short phrases which contain substrings of ``Mitch Landrieu''. Meanwhile, fully one fifth of the facets are slight varations of the string ``Sheriff Marlin Gusman". Such overlap clutters the UI with repetitive information and requires time and energy from users.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ c c }
 Raw & Heuristic cleanup  \\ 
 \hline
 Mayor Mitch & City Council  \\ 
 Mitch Landrieu & consent decree \\  
 Mayor Mitch Landrieu & Sheriff Marlin Gusman \\
 City Council & Department of Justice \\
 Mitch Landrieus & Police Department \\   
 New Orleans & Chief Administrative \\   
 Andy Kopplin & Mayor Mitch Landrieus \\
 consent decree & Ryan Berni \\    
 staff writer & Orleans Parish Prison \\   
 Sheriff Marlin & Andy Kopplin \\   
 Orleans Parish & District Court \\
 Landrieu administration & Landrieu spokesman \\  
 Sheriff Marlin Gusman & City Hall \\  
 Marlin Gusman & Landrieu Administration \\   

\end{tabular}
\caption{Top 15 facets for Q=Mitch Landrieu, in raw form and with heuristic cleanup}
\end{center}
\label{t1:one}
\end{table}

Thus, Rookie builds a list of facets, L, by iterating through the higest scoring ngrams one at a time. If an propsed facet, p, seems like a duplicate of $L_n$, Rookie will either replace $L_n$ with p (using heuristics to guide the decision to replace) or else skip over p entirely (without adding it to L). For instance, if p and $L_n$ have a Jaccard similarity that is greater than 50\% and p has more characters than $L_n$, Rookie will replace $L_n$ with p.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ l l }
 Facet problem & Example \\ 
 \hline
 Nested facets & Orleans Police \& New Orleans Police  \\ 
 Split facets & Mayor Mitch \& Mitch Landrieu \& Mayor Landrieu \\  
 Query-facet overlap & Q=Mitch Landrieu, F=Mayor Mitch \\
 Common term & Orleans Parish

\end{tabular}
\caption{Common facet problems}
\end{center}
\label{t2:two}
\end{table}

\subsection{Coreference}

Combining facets in this way could be said to be an extremely lightweight form of coreference resolution, a large topic within natural language processing, philosophy \footnote{Do ``Obamacare'' and the ``Affordable Care Act'' refer to the same thing?} and linguistics. Rookie's goal is more modest: simply finding some string which represents some facet of underlying documents, without confusing the user with duplicates. 

The motivation for this approach is twofold. First, this form of coreference resolution has proven suitable for this particular application. Second, coreference resolution methods are not currently fast enough to run run at query time in a user-facing application.

\subsection{Binning}

Some of the per-year facets are low on the global list but important for their year. ex. q=mitch landrieu t=Diana Bajoie, t=2012.

So take the top N facets for each yr by tfidf, merge into a query-wide facet list. then loop over top tfidf facets per bin again, adding facets from global list into the bin's facet list. this allows coherence between bin facets and global facets -- but also allows for important facets in a given bin.


\subsection{Performance, implementation \& benchmarking}

Rookie's facet algorithm is implemented in Python. It makes use of rapid matrix operations in Numpy. The speed of the facet algorithm varies with the size of D. When the size of D is around 300 (ex. Q=Mitch Landrieu) the algorithm runs in 60 milliseconds (ordinary clock speed) on a new laptop. When the size of D is 100 (ex. Q=levee) the algorithm runs in 30 milliseconds. 

A speedy facet algorithm is crucial for Rookie as single query from the UI might require five or ten calls to the facet engine, one for each date bin. \footnote{not discussed here}.

\end{document}
