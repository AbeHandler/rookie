\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{sectsty}
\usepackage{scrextend}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{nth}
\usepackage{pifont}
\usepackage{boxedminipage}
\usepackage[backend=bibtex]{biblatex} %backend tells biblatex what you will be using to process the bibliography file
\bibliography{additional.bib}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\usepackage{hyperref}
\usepackage{amsmath}

\sectionfont{\fontsize{12}{16}\selectfont}

\title{Data-driven methods for fine-grained, linguistically-informed, rapid summarization}
\author{Abram Handler}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
Current algorithms for document summarization (both query-focused summarization, and single/multi-document summarization) are poorly suited for user-facing systems. Approximate MMR methods are fast enough for interactive applications (linear with the size of the data), but sacrifice linguistic insight in favor of simple token-level tf-idf weights. \cite{mcdonald2007study}. Exact linear programming approaches use machine learning methods to find optimal summaries (?), but require hundreds of seconds of real time \cite{mcdonald2007study}. 

\section{From hand-written rules to MMR}

Early summarization used hand-written rules. These are pretty good -- but not data-driven. Condition on the particular word. Run ML at query time: expensive. Informed heuristics. Greedy.

\section{Events definition}

Filatova et. al use ``events'' to create summaries. Copying this approach requires a definition of an ``event''. Seems reasonable to say ``event'' is defined by the particular task at hand -- any other claim requires serious linguistics or cognitive psychology. One of their papers assumes 1 sentence = one event.


\vspace{.5cm}
\centerline{\begin{minipage}{0.7\textwidth} 
\textbf{Rookie-like idea}: noun phrase clusters? I suspect that finding high tf-idf noun phrases in sentences that contain Q or F or both will sketch out an event. Q = ``Mitch Landrieu'', F =``Marlin Gusman''. High tf-idf noun phrases: ``Department of Justice,'' ``Orleans Parish Prison''
\end{minipage}}
\vspace{.5cm}

\textbf{Temporal and spatial granularity}. Event based summarization for news has to deal with temporal and spatial granularity. Some of the Hovy papers address this. (1) The US has been fighting in Iraq since 2003. (2) In October, 2010 there was an ambush of US forces outside Fallujah. News reader project addressed this.

\textbf{Head words} Read Nate Chambers. more linguistics.

\section{Form of summary}

DUC and TREC summarization competitions have long focused on a particular form of single or multi-document summary. However, one could imagine that different choices about white space or text layout might make for more or less useful summaries. These choices about white space would in turn impact the choice of algorithm.

\begin{enumerate}\label{approach}
  \item \textbf{Search engine results} A form of summary.
  \item \textbf{Traditional summary (DUC/TREC)} I think: no line breaks. I think the goal is a human readable summary, like what an analyst would sit down to write. DUC website requires login. 
  \item \textbf{Bullet point summary} One short point per line. With a temporal corpus, these could be events. But for scientific summarization, each bullet point might represent a particular approach or theory. I think this is most sutable for Rookie.
  \item \textbf{KWIK viewer summary}
  \item \textbf{Sentence selection}: one sentence per line.
  \item \textbf{Sub-sentential clause or phrase selection}: one phrase per line, both with or without line breaks
\end{enumerate}

\section{Current Rookie}

Current Rookie, 3/15/16, takes approach \#5 defined in section \ref{approach}. After a search engine returns a list of documents in response to a query, Rookie places each sentence from each of the query-responsive documents, R (results), into exactly 1 of the following categories: 

\begin{enumerate}
  \item Sentence contains Q $\land$ F \footnote{If no F is selected, no sentences will be placed into category 1.}
  \item Sentence contains Q $\lor$ F
  \item Sentence $\neg$ contains Q $\lor$ F
\end{enumerate}

Each document will contribute N eligible sentences for potential inclusion in a multi document summary. Rookie chooses greedily from categories 1, 2, and 3 (in that order) until it finds the N sentences from a document. Rookie uses sentence position to break ties within a category, within a document. The set of all eligible sentences from R is called E.

Not all eligible sentences from E are displayed in the summary. Rookie's UI has a ``summarization area'' that is P pixels tall. The P pixels are divided into horizontal \textbf{stripes}. Each stripe, $s_i$ is filled with one sentence. The sentence is allowed to wrap within its stripe, so the height of one stripe $s_i$ may or may not equal the height of another, $s_j$. \footnote{Informally, the sentences in the summarization area should reflect important information about Q and F in T, although this notion is poorly defined.} Rookie picks sentences from E to fill the stripes until the height of the chosen stripes + the height of the next proposed stripe is greater than P. Rookie chooses sentences from E by picking greedily from categories 1, 2 and 3 (in that order). If a category contains more than one eligible sentence, Rookie chooses randomly.

This is not an anytime algorithm, but it is imperceptibly fast, even w/ React overhead. It could be turned into an anytime algorithm.

\vspace{.5cm}
\centerline{\begin{minipage}{0.7\textwidth} 
HCI + NLP note: this algorithm was designed to support ``interactive summarization'' along the axis of time. That is: a user might be interested in a particular ambush in Fallujah 2010, which is part of a broader US invasion from 2003-2015. ``Events-based summarization with context?'' Matching an event to its context seems like an unexplored problem. Pretty hard.
\end{minipage}}


\section{Paper ideas \& related work}

\subsection{Clause dropping and human readability}

Gramaticality vs. human readability. Are they the same? Chompsky example. ``Colorless green ideas sleep furiously.''

Human readable subgraphs
Query-focused sentence compression

Pittler 2010 defines sentence compression as producing a gramatical, shorter version of the original. Addition: compression with a set of objectives [Q, F, F' ...]. Tree-based method: edit the syntax tree. Sentence-based: generate strings.

Tree operations don't use dependencies.

Knight and Marcu 2002, call compression a scaled down version of summarization. Ziff-Davis corpus. Noisy channel model vs. tree operations.

Human readable subgraphs.

McDonald: issues w/ parse trees similar to Brendan's comments 3/21.

Linguistics: Galley and McKeown (2007) -- keep negations. McDonald: handles verbs.

Pittler (2010) Verb categories: give needs a subject, object and indirect object.

Clarke and Lapta (2007): discourse approaches. 

``A Sentence Compression Based Framework to Query-Focused
Multi-Document Summarization''

Hand-crafted rules, table 2. Wang 2013. CRF.

Query Relevance.

They don't really dig into why certain things work.

``Dependency Tree Based Sentence Compression''
Good idea: condition on words. So you delete based on the word. Removing the direct object/indirect object for ``gave''
is bad: ``Mitch Landrieu gave tax breaks to the low-income home owners''. But... can't think of case when ok to remove DO actually. 

Easily Turkable.

5 Participants, 25 sentences. Human readability. Replicate w/ tons of data.

``Snippet generation''
Does it need to make sense?

What rules are OK? What rules are not? 

Do you get a sentence out? Do you get a clause out? What clauses are OK? What subgraphs make sense in snippets? What subgraphs make sense jammed toegether? ``Barack Obama traveled to Cuba... freed the dissidents in 2016''. If you had periods betweeen the clauses that would help.

``Using tweets to help sentence compression for news highlights generation''

Semantic role labeling + prop bank.

\section{People who cite Dependency Tree Based Sentence Compression}

Wikipedia and simple wikipedia. 

Empirically: what rules are good? For what? For compression?


\section{Evaluation}

Brendan 3/15: I think we'll need to compare alternative summaries side-by-side and think about what information they convey, relative to the user's expressed information need (which I think is the (Q,F,T) tuple).  then eventually operationalize our criteria into an mturk-able (or otherwise annotatable) task.

\section{Results}

Brendan 3/15: The failure of ``naive multidoc snippets", if I understand it correctly, is a good finding -- it's a great argument for joint inference being important, since joint inference helps ensure the selected text spans give the right context for each other.

\section{A ``judgments'' evaluation}
One use case for summarization: should I read this? A good summary helps you make the correct choice about reading (as opposed to best reflecting the contents of the document). If you have a particular reason for being interested in a document (a Q or F) then maybe these are different goals. There are lots of IR corpora about ``judgments" (i.e. and expert says which docs are relevant or not relevant), which might be a way to approach this.

\section{Related work}

Filatova

Toward Abstractive Summarization Using Semantic Representations

Event-Centric Summary Generation 

Talk to Emma and Luke who wrote a paper on this.

\printbibliography

\end{document}
