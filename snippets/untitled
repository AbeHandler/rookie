        # some score keeping for the model to be cleaned up later. TODO Scorekeeper is a different function
        score_keeping = []
        all_tokens = []
        for sentence in document.keys():
            sentence_tokens = document[sentence]['tokens']
            for word_no in sentence_tokens.keys():
                word = document[sentence]['tokens'][word_no]
                all_tokens.append((word['word'], word['z']))
        for w in doc_vocab_counter:
            isquery = sum(1 for t in all_tokens if t[0] == w and t[1] == "Q")
            score_keeping.append((w, doc_vocab_counter[w], isquery))
        grand_total_score_keeping["Q"].append(score_keeping)

        score_keeping = []
        for w in doc_vocab_counter:
            isquery = sum(1 for t in all_tokens if t[0] == w and t[1] == "D")
            score_keeping.append((w, doc_vocab_counter[w], isquery))
        grand_total_score_keeping["D"].append(score_keeping)

pdb.set_trace()

'''
Score keeping and debugging
'''

most_common_labeled_q = []
joined = [i for i in itertools.chain(*grand_total_score_keeping["Q"])]
for word in doc_vocab_counter:
    tmp = [o for o in joined if o[0] == word]
    total_occurances = float(sum(i[1] for i in tmp))
    total_occurances_zs = float(sum(i[2] for i in tmp))
    if total_occurances_zs == 0:
        most_common_labeled_q.append((word, 0))
    else:
        most_common_labeled_q.append((word, total_occurances_zs/total_occurances))
most_common_labeled_q.sort(key=lambda x: x[1])


most_common_labeled_d = []
joined = [i for i in itertools.chain(*grand_total_score_keeping["D"])]
for word in doc_vocab_counter:
    tmp = [o for o in joined if o[0] == word]
    total_occurances = float(sum(i[1] for i in tmp))
    total_occurances_zs = float(sum(i[2] for i in tmp))
    if total_occurances_zs == 0:
        most_common_labeled_d.append((word, 0))
    else:
        most_common_labeled_d.append((word, total_occurances_zs/total_occurances))
most_common_labeled_d.sort(key=lambda x: x[1])


def get_pct_of_sentence(sentence_tokens, target):
    assert (target == "D" or target=="Q" or target=="G")
    total = 0
    target_total = 0
    for key in sentence_tokens.keys():
        total += 1
        zval = sentence_tokens[key]['z'] 
        if zval == target:
            target_total +=1
    if total == 0:
        return 0
    else:
        return float(target_total)/float(total)


q_label_good = []
for sentence in document.keys():
    frac = get_pct_of_sentence(document[sentence]['tokens'], "Q")
    sente = " ".join([i.raw for i in inf.doc.sentences[sentence].tokens])
    q_label_good.append((frac, sente))

q_label_good.sort(key=lambda x: x[0])

pdb.set_trace()

plt.scatter(range(0, len(z_flips_counts)), z_flips_counts)
plt.title('log z flips per iteration')
plt.ylabel('log z flips')
plt.ylabel('document iteration')
plt.show()
